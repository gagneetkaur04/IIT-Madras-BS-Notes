{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMHrw0FmXtwqCYMjjpUq3IX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# DATA\n","\n","X_train = []\n","y_train = []\n","X_test = []\n","y_test = []"],"metadata":{"id":"ov_ebwv0kieb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Polynomial Regression**"],"metadata":{"id":"1euhupqFS-vA"}},{"cell_type":"markdown","source":["### How is polynomial regression model trained?"],"metadata":{"id":"ZJ4bfPutS5P4"}},{"cell_type":"markdown","source":["1. Apply `polynomial transformation` on the feature matrix.\n","2. Learn `linear regression model` (via normal equation or SGD) on the transformed feature matrix."],"metadata":{"id":"m2UVHOL9TDW0"}},{"cell_type":"markdown","source":["**Implementation tips :** Make use of pipeline construct for polynomial transformation followed by linear regression estimator"],"metadata":{"id":"mHHVoW1opwr3"}},{"cell_type":"markdown","source":["1. Set up polynomial regression model with normal equation"],"metadata":{"id":"tRKnSx_TqDIv"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"gckerNI3RIqe"},"outputs":[],"source":["from sklearn.linear_model import LinearRegression\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import PolynomialFeatures\n","\n","poly_model = Pipeline([('polynomial_transform', PolynomialFeatures(degree = 2)),\n","                       ('LR', LinearRegression)])\n","\n","poly_model.fit(X_train, y_train)"]},{"cell_type":"markdown","source":["2. Set up polynomial regression model with SGD"],"metadata":{"id":"RGU-z4XeqoK8"}},{"cell_type":"code","source":["from sklearn.linear_model import SGDRegressor\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import PolynomialFeatures\n","\n","poly_model = Pipeline([('polynomial_transform', PolynomialFeatures(degree = 2)),\n","                       ('SGD', SGDRegressor)])\n","\n","poly_model.fit(X_train, y_train)"],"metadata":{"id":"LBlb04EnqzIv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### How to use only interaction features for polynomial regression?"],"metadata":{"id":"QXBzlA0Qq7Ab"}},{"cell_type":"markdown","source":["[ $ùìß_1$ , $ùìß_2$ ] is transformed to [ 1, $ùìß_1$ , $ùìß_2$ , $ùìß_1$$ùìß_2$ ]\n","\n","Note that [ $ùìß_1^2$ , $ùìß_2^2$ ] are excluded"],"metadata":{"id":"d-s9zWP_rRRC"}},{"cell_type":"code","source":["from sklearn.preprocessing import PolynomialFeatures\n","poly_transform = PolynomialFeatures(degree = 2, interaction_only = True)"],"metadata":{"id":"qMBigI4HrBoc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Regularization**"],"metadata":{"id":"0BmX2Aexdt6T"}},{"cell_type":"markdown","source":["## RIDGE"],"metadata":{"id":"X5E4iOmplGfu"}},{"cell_type":"markdown","source":["### How to perform ridge regularization with specific regularization rate?"],"metadata":{"id":"sBFRHKradxoB"}},{"cell_type":"markdown","source":["**Option 1**\n","\n","Step1: Instantiate object of `Ridge` estimator\n","\n","Step2: Set parameter `alpha` to the required regularization rate\n","\n","`fit`, `score`, `predict` work exactly like other linear regression estimators"],"metadata":{"id":"8Xek-b1XeRot"}},{"cell_type":"code","source":["from sklearn.linear_model import Ridge\n","ridge = Ridge(alpha = 1e-3)"],"metadata":{"id":"r998_GB8dw3X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Option 2**\n","\n","Step1: Instantiate object of `SGDRegressor` estimator\n","\n","Step2: Set parameter `alpha` to the required regularization rate  and `penalty = l2`"],"metadata":{"id":"5jygohNoe2Oy"}},{"cell_type":"code","source":["from sklearn.linear_model import SGDRegressor\n","sgd = SGDRegressor(alpha = 1e-3 , penalty = 'l2')"],"metadata":{"id":"Eivzthtne_D5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### How to search the best regularization parameter for ridge?"],"metadata":{"id":"NhkDEWEXfMe_"}},{"cell_type":"markdown","source":["**Option 1**\n","\n","Search for the best regularization rate with built-in cross validation in `RidgeCV` estimator\n","\n","**Option 2**\n","\n","Use cross validation with `Ridge` or `SGDRegressor` to search for best regularization\n","  * Grid search\n","  * Randomized search"],"metadata":{"id":"fMM9I4eZfSBi"}},{"cell_type":"markdown","source":["### How to perform ridge regularization in polynomial regression?"],"metadata":{"id":"_vHEvnpSjsea"}},{"cell_type":"markdown","source":["Set up a pipeline of polynomial transformation followed by ridge regressor."],"metadata":{"id":"UE_rMy4Wj1Ai"}},{"cell_type":"code","source":["from sklearn.linear_model import Ridge\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import PolynomialFeatures\n","\n","poly_model = Pipeline([('polynomial_transform', PolynomialFeatures(degree = 2)),\n","                       ('ridge', Ridge(alpha = 1e-3))])\n","\n","poly_model.fit(X_train, y_train)"],"metadata":{"id":"e2_ER0OIfRQj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Instead of `Ridge`, we can use SGDRegressor to get equivalent formulation."],"metadata":{"id":"M5TtbceHkrQ7"}},{"cell_type":"markdown","source":["## LASSO"],"metadata":{"id":"Yy8iZUY8lMTj"}},{"cell_type":"markdown","source":["### How to perform LASSO regularization with specific regularization rate?"],"metadata":{"id":"QaTRSO45k4Tt"}},{"cell_type":"markdown","source":["**Option 1**\n","\n","Step1: Instantiate object of `Lasso` estimator\n","\n","Step2: Set parameter `alpha` to the required regularization rate\n","\n","`fit`, `score`, `predict` work exactly like other linear regression estimators"],"metadata":{"id":"0NSMAYhGlVLH"}},{"cell_type":"code","source":["from sklearn.linear_model import Lasso\n","lasso = Lasso(alpha = 1e-3)"],"metadata":{"id":"-sRCZ6Q2lVLH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Option 2**\n","\n","Step1: Instantiate object of `SGDRegressor` estimator\n","\n","Step2: Set parameter `alpha` to the required regularization rate  and `penalty = l1`"],"metadata":{"id":"6AY0tFm0lVLH"}},{"cell_type":"code","source":["from sklearn.linear_model import SGDRegressor\n","sgd = SGDRegressor(alpha = 1e-3 , penalty = 'l1')"],"metadata":{"id":"U0rLqQDylVLH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### How to search the best regularization parameter for lasso?"],"metadata":{"id":"egCids3Xl4Zq"}},{"cell_type":"markdown","source":["**Option 1**\n","\n","Search for the best regularization rate with built-in cross validation in `LassoCV` estimator\n","\n","**Option 2**\n","\n","Use cross validation with `Lasso` or `SGDRegressor` to search for best regularization\n","  * Grid search\n","  * Randomized search"],"metadata":{"id":"SdwmBCEyl4Zx"}},{"cell_type":"markdown","source":["### How to perform lasso regularization in polynomial regression?"],"metadata":{"id":"crgYL2cYmQFF"}},{"cell_type":"markdown","source":["Set up a pipeline of polynomial transformation followed by lasso regressor."],"metadata":{"id":"_LF_NS8nmQFF"}},{"cell_type":"code","source":["from sklearn.linear_model import Lasso\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import PolynomialFeatures\n","\n","poly_model = Pipeline([('polynomial_transform', PolynomialFeatures(degree = 2)),\n","                       ('lasso', Lasso(alpha = 1e-3))])\n","\n","poly_model.fit(X_train, y_train)"],"metadata":{"id":"Qcqiq_f6mQFF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Instead of `Lasso`, we can use SGDRegressor to get equivalent formulation."],"metadata":{"id":"GG9r9PkMmQFF"}},{"cell_type":"markdown","source":["## Ridge and Lasso together"],"metadata":{"id":"QJvr-pIRmfiX"}},{"cell_type":"markdown","source":["### How to perform both lasso and ridge regularization in polynomial regression?"],"metadata":{"id":"SiBKGTMOmmgz"}},{"cell_type":"markdown","source":["Set up a pipeline of polynomial transformation followed by the SGDRegressor with `penalty = 'elasticnet'`"],"metadata":{"id":"Z07EUUJ-mvvW"}},{"cell_type":"code","source":["from sklearn.linear_model import Lasso\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import PolynomialFeatures\n","\n","poly_model = Pipeline([('polynomial_transform', PolynomialFeatures(degree = 2)),\n","                       ('elasticnet', SGDRegressor(penalty = 'elasticnet', l1_ration = 0.3))])\n","\n","poly_model.fit(X_train, y_train)"],"metadata":{"id":"BFR7u6_9miv8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["`elasticnet` is a convex combination of L1(Lasso) and L2(Ridge) regularization.\n","\n","In the above example, we have set `l1_ratio = 0.3` which means `l2_ratio = 1 - l1_ratio = 0.7` . L2 takes higher weightage in this formulation."],"metadata":{"id":"JvX-rxRGo4vS"}},{"cell_type":"markdown","source":["# **Hyperparameter Tuning**"],"metadata":{"id":"c5j8Kbf6sIh_"}},{"cell_type":"markdown","source":["### How to recognize hyperparameters in any sklearn estimator?"],"metadata":{"id":"Xfc0cZL6wvFZ"}},{"cell_type":"markdown","source":["* `Hyper-parameters` are parameters that are not directly learnt within estimators.\n","\n","* In `sklearn`, they are passed as arguments to the constructor of the estimator classes.\n","\n","* For example,\n","\n","  * `degree` in `PolynomialFeatures`\n","  * `learning_rate` in `SGDRegressor`"],"metadata":{"id":"xdWkcgLtw1uS"}},{"cell_type":"markdown","source":["### How to set these hyperparameters?"],"metadata":{"id":"WlOkIy1Gx-gA"}},{"cell_type":"markdown","source":["* Select hyperparameters that results in the best cross validation scores.\n","\n","* Hyper parameter search consists of:\n","\n","  * an estimator (regressor or classifier)\n","  * a parameter space\n","  * a method for searching or sampling candidates\n","  * a cross-validation scheme\n","  * a score function\n","\n","* We can specify hyperparameter search with these five components."],"metadata":{"id":"0dB9d-hWyBmP"}},{"cell_type":"markdown","source":["Two generic HPT approaches implemented in sklearn are:\n","\n","  * `GridSearchCV` exhaustively considers all parameter combinations for specified values.\n","\n","  * `RandomizedSearchCV` samples a given number of candidate values from a parameter space with a specified distribution."],"metadata":{"id":"mQXgEHMGyuJB"}},{"cell_type":"code","source":["# GridSearchCV\n","\n","param_grid = [\n","    {'C': [1,10,100,1000],\n","     'kernel' : ['linear']}\n","]"],"metadata":{"id":"kiP6_79vsL8b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# RandomizedSearchCV\n","\n","param_dist = {\n","    'average': [True, False],\n","    'l1_ratio': stats.uniform(0,1),\n","    'alpha': loguniform(1e-4, 1e0)\n","}"],"metadata":{"id":"bTkEUFDtzo3X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### What are the differences between grid and randomized search?"],"metadata":{"id":"39yk6qgD0Q0k"}},{"cell_type":"markdown","source":["Grid Search\n","  * Specifies exact values of parameters in grid\n","\n","Randomized Search\n","  * Specifies distributions of parameter values and values are sampled from those distributions.\n","  * Computational budget can be chosen independent of number of parameters and their possible values.\n","  * The budget is chosen in terms of the number of sampled candidates or the number of training iterations specified in `n_iter` argument."],"metadata":{"id":"zx09UXiE0gnA"}},{"cell_type":"markdown","source":["### What data split is recommended for HPT?"],"metadata":{"id":"bDm7Bf8V1QPd"}},{"cell_type":"markdown","source":["**STEP-1**\n","\n","* Divide training data into `training`, `validation` and `test` sets.\n","\n","**STEP-2**\n","\n","* For each combination of hyper-parameter values learn a model with training set.\n","* This step create multiple models.\n","* This step can be run in parallel by setting `n_jobs = -1`\n","* Some parameter combinations may cause failure in fitting one or more folds of data. This may cause the search to fail. Set `error_score = 0` (or np.NaN) to set score for the problematic fold to 0 and complete the search.\n","\n","**STEP-3**\n","\n","* Evaluate performance of each model with validation set and select a model with the best evaluation score.\n","\n","**STEP-4**\n","\n","* Retrain model with the best hyper-parameter settings on training and validation set combined.\n","\n","**STEP-5**\n","\n","* Evaluate the model performance on the test set.\n","\n","Note : The test set was not used in hyper-parameter search and model retraining. That is why this performance measure is likely to give us true performance measure on the unseen data.\n","\n","\n"],"metadata":{"id":"bec49FfS1WTt"}},{"cell_type":"markdown","source":["### What are some of the model specific HPT available for regression tasks?"],"metadata":{"id":"mkhSXyL33-_d"}},{"cell_type":"markdown","source":["* Some models can fit data for a range of values of some parameter almost as efficiently as fitting the estimator for a single value of the parameter.\n","\n","* This feature can be leveraged to perform more efficient cross-validation used for model selection of this parameter.\n","\n","  * linear_model.LassoCV\n","  * linear_model.RidgeCV\n","  * linear_model.ElasticNetCV"],"metadata":{"id":"vXJ64YOl7dYl"}},{"cell_type":"markdown","source":["### How to determine degree of polynomial regression with grid search?"],"metadata":{"id":"K7arHMAY8mck"}},{"cell_type":"code","source":["from sklearn.model_selection import GridSearchCV\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import PolynomialFeatures\n","from sklearn.linear_model import SGDRegressor\n","\n","param_grid = [\n","    {\n","        'poly__degree' : [2,3,4,5,6,7,8,9]\n","    }\n","]\n","\n","pipeline = Pipeline(steps = [\n","    ('poly', PolynomialFeatures()),\n","    ('sgd', SGDRegressor())\n","])\n","\n","grid_search = GridSearchCV(pipeline, param_grid, cv = 5, scoring = 'neg_mean_sqaured_error', return_train_score = True)\n","\n","grid_search.fit(X_train.reshape(-1,1), y_train)"],"metadata":{"id":"ahKLwFKp5sne"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **California Housing Dataset**"],"metadata":{"id":"iNtfkIwm9-X-"}},{"cell_type":"markdown","source":["Basic Understanding of Data"],"metadata":{"id":"afseIHUHCWXm"}},{"cell_type":"code","source":["from sklearn.datasets import fetch_california_housing"],"metadata":{"id":"4vIDX5_5-BpD","executionInfo":{"status":"ok","timestamp":1689448476566,"user_tz":-330,"elapsed":396,"user":{"displayName":"Gagneet Kaur","userId":"01282326010698350277"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["ch = fetch_california_housing(as_frame = True)"],"metadata":{"id":"aYYEV0MV-etj","executionInfo":{"status":"ok","timestamp":1689448680246,"user_tz":-330,"elapsed":2127,"user":{"displayName":"Gagneet Kaur","userId":"01282326010698350277"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["Linear Regression on California Housing Dataset"],"metadata":{"id":"twp7whP3CZBK"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","\n","from scipy.stats import loguniform, uniform\n","\n","from sklearn.dummy import DummyRegressor\n","\n","from sklearn.linear_model import LinearRegression, Lasso, LassoCV, Ridge, RidgeCV, SGDRegressor\n","\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n","\n","from sklearn.model_selection import cross_validate, cross_val_score, train_test_split, ShuffleSplit, validation_curve, GridSearchCV, RandomizedSearchCV\n","\n","from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n","\n","from sklearn.pipeline import Pipeline"],"metadata":{"id":"d0BXLAb3BZW5","executionInfo":{"status":"ok","timestamp":1689449846976,"user_tz":-330,"elapsed":382,"user":{"displayName":"Gagneet Kaur","userId":"01282326010698350277"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["cv = ShuffleSplit(n_splits = 10, test_size = 0.2, random_state = 42)"],"metadata":{"id":"aSicTiXKDtSB","executionInfo":{"status":"ok","timestamp":1689449921741,"user_tz":-330,"elapsed":407,"user":{"displayName":"Gagneet Kaur","userId":"01282326010698350277"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["#DATA LOADING AND SPLITTING\n","\n","features, labels = fetch_california_housing(as_frame = True, return_X_y = True)\n","\n","com_train_features, X_test, com_train_labels, y_test = train_test_split(features, labels, random_state = 42)\n","\n","X_train, X_dev, y_train, y_dev = train_test_split(com_train_features, com_train_labels, random_state = 42)"],"metadata":{"id":"0GBsq9wtD_iX","executionInfo":{"status":"ok","timestamp":1689450166190,"user_tz":-330,"elapsed":392,"user":{"displayName":"Gagneet Kaur","userId":"01282326010698350277"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Po9F7h8oE7Nl"},"execution_count":null,"outputs":[]}]}