{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNa7sJirHGPYPYvH49/a+y0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Linear Regression**"],"metadata":{"id":"CHKgeNPBeym0"}},{"cell_type":"code","source":["# DATA\n","\n","X_train = []\n","y_train = []\n","X_test = []\n","y_test = []"],"metadata":{"id":"TxhW6eB3lZDd","executionInfo":{"status":"ok","timestamp":1689351545154,"user_tz":-330,"elapsed":457,"user":{"displayName":"Gagneet Kaur","userId":"01282326010698350277"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["### **How to build baseline regression model**"],"metadata":{"id":"i5qcD19Je2od"}},{"cell_type":"markdown","source":["`Dummy Regressor` helps in creating a 'baseline' for regression."],"metadata":{"id":"qNg72dJee5c_"}},{"cell_type":"code","source":["from sklearn.dummy import DummyRegressor"],"metadata":{"id":"9FOzK0DNe3Fn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dummy_regr = DummyRegressor(strategy = 'mean')\n","dummy_regr.fit(X_train, y_train)\n","dummy_regr.predict(X_test)\n","dummy_regr.score(X_test, y_test) #score returns R^2 or coeff of determination"],"metadata":{"id":"9lu7yMGMe8yj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* It makes a prediction as specified by the `strategy`\n","* Strategy is based on some statistical property of the training set or user specified value.\n","\n","  `Strategy = ['mean', 'median', 'quantile', 'constant']`"],"metadata":{"id":"P3n9buepfHlP"}},{"cell_type":"markdown","source":["### **How is Linear Regression model trained?**"],"metadata":{"id":"6HMqmBqefiMI"}},{"cell_type":"markdown","source":["**Step-1** : Instantiate `object` of a suitable linear regression estimator from one of the following two options:\n","  * Normal Equation (`LinearRegression()`)\n","  * Iterative Optimization (`SGDRegressor()`)\n"],"metadata":{"id":"Nwi4IMH5jSsn"}},{"cell_type":"code","source":["# NORMAL EQUATION\n","\n","from sklearn.linear_model import LinearRegression\n","LR = LinearRegression()"],"metadata":{"id":"8d5kvtswfdoF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ITERATIVE OPTIMIZATON\n","\n","from sklearn.linear_model import SGDRegressor\n","LR = SGDRegressor()"],"metadata":{"id":"aDjwFPRCkk2w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Step-2** : Call `fit` method on linear regression object with training feature matrix and label vector as arguments\n","\n","Both the _feature matrix_ and _label vector_ comes from the **TRAINING SET**\n","\n","Works for both single and multi-output regression"],"metadata":{"id":"j1LQ1rPhk1jP"}},{"cell_type":"code","source":["# Model training with feature matrix 'X_train' and label vector or matrix 'y_train'\n","\n","LR.fit(X_train, y_train)"],"metadata":{"id":"WlvGI7mPlGID"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **SGDRegressor Estimator**"],"metadata":{"id":"M2uGOE2gEOD2"}},{"cell_type":"markdown","source":["* Implements stochastic gradient descent\n","* Use for large training set up (>10k samples)\n","* Provides greater control on optimization process through provision for hyperparameter settings.\n","\n","  * **Loss Parameter:**\n","    * `loss = 'squared error'`\n","    * `loss = 'huber'`\n","\n","  * **Penalty parameter:**\n","    * `penalty = 'l1'`\n","    * `penalty = 'l2'`\n","    * `penalty = 'elasticnet'` _(it is a convex combination of l1 and l2 regularization)_\n","\n","  * **Learning Rate parameter**\n","    * `learning_rate = 'constant'`\n","    * `learning_rate = 'optimal'`\n","    * `learning_rate = 'invscaling'` _(default)_\n","    * `learning_rate = 'adaptive'`\n","\n","  * **Early Stopping** _(helps us to stop the iterations of SGDRegressor)_\n","    * `early_stopping = 'True'`\n","    * `early_stopping = 'False'`\n","\n","\n"],"metadata":{"id":"m4hZ256sEhcX"}},{"cell_type":"markdown","source":["##### Random Seed"],"metadata":{"id":"9CgeUG_0G2Ex"}},{"cell_type":"markdown","source":["* The random seed is a starting point for generating a sequence of random numbers. Setting a random seed ensures that the same sequence of random numbers is generated every time you run your code.\n","\n","* It's a good idea to use a **random seed** of your choice while instantiating SGDRegressor object. It helps us get reproducible results.\n","\n","* Set `random_state` to seed of your choice."],"metadata":{"id":"H5anby85G-rz"}},{"cell_type":"code","source":["from sklearn.linear_model import SGDRegressor\n","LR = SGDRegressor(random_state = 30)"],"metadata":{"id":"LLk-GyygEVPg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### How to perform feature scaling for SGDRegressor?"],"metadata":{"id":"ie4NUnBpH97v"}},{"cell_type":"markdown","source":["* SGD is sensitive to feature scaling, so it is highly recommended to scale input feature matrix."],"metadata":{"id":"mZ6nrziZIQRW"}},{"cell_type":"code","source":["from sklearn.linear_model import SGDRegressor\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import StandardScaler\n","\n","sgd = Pipeline([\n","    ('feature_scaling', StandardScaler()),\n","    ('sgd_regressor', SGDRegressor())\n","])\n","\n","sgd.fit(X_train, y_train)"],"metadata":{"id":"ebMxgIs6IPZH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* Feature scaling is not needed for word frequencies and indicator features as they have intrinsic scale\n","\n","* Features extracted using PCA should be scaled by some constant c such that the average L2 norm of the training data equals 1."],"metadata":{"id":"USecf2CkKFtD"}},{"cell_type":"markdown","source":["##### How to shuffle training data after each epoch in SGDRegressor?"],"metadata":{"id":"t-_p83H1KxF6"}},{"cell_type":"code","source":["from sklearn.linear_model import SGDRegressor\n","LR = SGDRegressor(shuffle = True) #data will be shuffled before every epoch in the SGDRegressor"],"metadata":{"id":"dz0RFf5LKqhP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### How to set learning rate in SGDRegressor?"],"metadata":{"id":"2jYJPh08LYmt"}},{"cell_type":"markdown","source":["* Three different learning rates:\n","\n","  `learning_rate = ['constant', 'invscaling', 'adaptive']`\n","\n","* Default Setting:\n","\n","  `learning_rate = 'invscaling' , eta0 = 1e-2 , power_t = 0.25`\n","\n","* Learning Rate reduces after every iteration:\n","\n","   η = $\\frac {η_0} {t^{power-t}} $\n","\n","* You can make changes to these parameters to speed up or slow down the training process"],"metadata":{"id":"NkYN0OAqMAwM"}},{"cell_type":"markdown","source":["##### How to set constant learning rate?"],"metadata":{"id":"RSLI6gP9VgHz"}},{"cell_type":"code","source":["from sklearn.linear_model import SGDRegressor\n","LR = SGDRegressor( learning_rate = 'constant', eta0 = 1e-2)"],"metadata":{"id":"dXXOJI9mLygN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* Constant learning rate `eta0 = 1e-2` is used throughout the training"],"metadata":{"id":"f7UPOIJbV18N"}},{"cell_type":"markdown","source":["##### How to set adaptive learning rate?"],"metadata":{"id":"fzBzSuYkWG5P"}},{"cell_type":"code","source":["from sklearn.line_model import SGDRegressor\n","LR = SGDRegressor(learning_rate = 'adaptive', eta0 = 1e-2)"],"metadata":{"id":"MhKAX-aSWLSM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* The learning rate is kept to initial value as long as the training loss decreases.\n","\n","* When the stopping criterion is reached, the learning rate is divided by 5, and the training loop continues.\n","\n","* The algorithm stops when the learning rate goes below ${10^{-6}}$"],"metadata":{"id":"JOfSmIdWWYEP"}},{"cell_type":"markdown","source":["##### How to set number of epochs in SGDRegressor?"],"metadata":{"id":"CcvQns40Xdml"}},{"cell_type":"markdown","source":["* Set max_iter to desired number of epochs. The default value is 1000."],"metadata":{"id":"S2aKgtLUYjZW"}},{"cell_type":"code","source":["from sklearn.linear_model import SGDRegressor\n","LR = SGDRegressor(max_iter = 100)"],"metadata":{"id":"Ig4As5DdXc9O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* Remember one epoch is one full pass over the training data.\n","\n","* SGD converges after observing approximately $10^6$ training samples. Thus a reasonable first guess for the number of iterations for n sampled training set is\n","  \n","  `max_iter = np.ceil( 10^6 / n)`"],"metadata":{"id":"ynWbpspXY1bP"}},{"cell_type":"markdown","source":["##### How to use set stopping criteria in SGDRegressor?"],"metadata":{"id":"4aULKhuzZiMG"}},{"cell_type":"markdown","source":["* Option-1:\n","\n","  `tol` , '`n_iter_no_change` , `max_iter`\n","\n","  Here the SGDRegressor stops\n","  * when the training loss does not improve (loss > best_loss --> `tol`) for `n_iter_no_change` consecutive epochs\n","  * else after a maximum number of iteration `max_iter`"],"metadata":{"id":"YuP5cfN8Zs0a"}},{"cell_type":"code","source":["from sklearn.linear_model import SGDRegressor\n","LR = SGDRegressor(loss = 'squared_error', max_iter = 500, tol = 1e-3, n_iter_no_change = 5)"],"metadata":{"id":"lmiKgJOFZcAh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* Option-2:\n","\n","  `early_stopping`, `validation_fraction`\n","\n","  * Set aside `validation_fraction` percentage records from training set as validation set. Use `score` method to obtain validation score.\n","  \n","  * The SGDRegressor stops when\n","    \n","    * validation score does not improve by at least `tol` for `n_iter_no_change` consecutive epochs.\n","\n","    * else after a maximum number of iteration `max_iter`\n","\n"],"metadata":{"id":"LqvsSaE3aN8q"}},{"cell_type":"code","source":["from sklearn.linear_model import SGDRegressor\n","LR = SGDRegressor(loss = 'squared_error', early_stopping = True, max_iter = 500, tol = 1e-3, validation_fraction = 0.2, n_iter_no_change = 5)"],"metadata":{"id":"71MjwPfDCvbX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### How to use different loss functions in SGDRegressor?"],"metadata":{"id":"V952LPcwEsl6"}},{"cell_type":"markdown","source":["* Set `loss` parameter to one of the supported values\n","\n","  `squared_error`"],"metadata":{"id":"SobWaJf8E0Rr"}},{"cell_type":"code","source":["from sklrean.linear_model import SGDRegressor\n","LR = SGDRegressor(loss = 'sqaured_error')"],"metadata":{"id":"PiU3LfIiEztA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### How to use averaged SGD?"],"metadata":{"id":"1LLkBkP2FZ1F"}},{"cell_type":"markdown","source":["Averaged SGD updates the weight vector to _average of weights_ from previous updates."],"metadata":{"id":"7VuUDMI-Fdk4"}},{"cell_type":"markdown","source":["* Option-1\n","\n","  Averaging cross all updates `average = True`"],"metadata":{"id":"lsUWSYjPFneJ"}},{"cell_type":"code","source":["from sklearn.linear_model import SGDRegressor\n","LR = SGDRegressor( average = True )"],"metadata":{"id":"I2f0yoyRFc0k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* Option-2\n","\n","  * Set `average` to int value\n","\n","  * Averaging begins once the total number of samples seen reaches `average`\n","\n","  * Setting `average = 10` starts averaging after seeing 10 samples"],"metadata":{"id":"i-uw-FwoHSwa"}},{"cell_type":"code","source":["from sklearn,linear_model import SGDRegressor\n","LR = SGDRegressor( average = 10)"],"metadata":{"id":"_6rh-WG4IJ1Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Averaged SGD works best with a larger number of features and a higher `eta0`"],"metadata":{"id":"ooQbbqhHIRRF"}},{"cell_type":"code","source":[],"metadata":{"id":"N4GU5UmnIbU4"},"execution_count":null,"outputs":[]}]}